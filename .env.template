# NCL Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# REQUIRED: User Input Needed (No Defaults)
# =============================================================================

# Supabase Configuration
# Get these from: https://supabase.com/dashboard → Project Settings
#
# SUPABASE_URL: Project URL (Settings → API → Project URL)
#   Example: https://abcdefghijkl.supabase.co
#
# SUPABASE_KEY: Service role key for backend operations (Settings → API → service_role secret)
#   WARNING: Keep this secret! Never expose in frontend code
#   Example: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
#
# SUPABASE_DB_URL: Direct PostgreSQL connection (Settings → Database → Connection string → URI)
#   Use the "Transaction pooler" connection string (port 5432) for best compatibility.
#   If you have IPv4 connectivity issues, use the "Session pooler" (port 5432) instead of direct connection.
#   Example: postgresql://postgres.[project-ref]:[password]@aws-0-[region].pooler.supabase.com:5432/postgres
#
SUPABASE_URL=
SUPABASE_KEY=
SUPABASE_DB_URL=

# OpenAI Configuration
# Get this from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# JWT Secret for API Authentication
# Get this from: Supabase Dashboard → Project Settings → API → JWT Secret
SUPABASE_JWT_SECRET=

# Cohere API Key (optional, but recommended for better search accuracy)
# Get this from: https://dashboard.cohere.com/api-keys
# Set RERANK_ENABLED=false below if you don't want to use reranking
COHERE_API_KEY=

# LlamaParse API Key (optional, for legacy document formats)
# Get this from: https://cloud.llamaindex.ai/api-key
# Enables support for: .doc, .xls, .ppt, .rtf, .csv, .odt, .ods, .odp
LLAMA_CLOUD_API_KEY=

# =============================================================================
# Model Configuration
# =============================================================================
# Each functionality can use a dedicated model. If not set, falls back to LLM_MODEL.
#
# Model selection guide:
#   - gpt-4o-mini  : Fast, cheap, 128k context. Good for simple tasks.
#   - gpt-4o       : Smarter, more expensive, 128k context. Better reasoning.
#   - gpt-4-turbo  : High quality, 128k context. Best for complex analysis.
#   - claude-3-haiku: Fast, cheap via LiteLLM. Good for simple extraction.
#   - claude-3-sonnet: Balanced speed/quality via LiteLLM.
# =============================================================================

# Embedding model for vector search
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=1536

# Default LLM model (fallback when specific model not set)
LLM_MODEL=gpt-4o-mini

# -----------------------------------------------------------------------------
# Context Generation (summarizes documents for better retrieval)
# Task: Generate 2-3 sentence summaries of emails/documents
# Complexity: Low-Medium | Speed: Important (runs per document)
# Recommendation: Use fast/cheap model, fallback handles edge cases
# -----------------------------------------------------------------------------
CONTEXT_LLM_MODEL=gpt-4o-mini
CONTEXT_LLM_FALLBACK=gpt-4o
CONTEXT_LLM_MAX_TOKENS=120000

# -----------------------------------------------------------------------------
# Email Cleaner (extracts meaningful content, removes signatures/boilerplate)
# Task: Identify content boundaries in email threads
# Complexity: Low | Speed: Important (runs per email)
# Recommendation: Fast model sufficient, simple boundary detection
# -----------------------------------------------------------------------------
EMAIL_CLEANER_MODEL=gpt-4o-mini

# -----------------------------------------------------------------------------
# Image Processing (classifies and describes images)
# Task: Determine if image is content vs logo/signature, generate descriptions
# Complexity: Medium | Speed: Moderate (runs per image attachment)
# Recommendation: Vision-capable model required (gpt-4o-mini has vision)
# -----------------------------------------------------------------------------
IMAGE_LLM_MODEL=gpt-4o-mini

# -----------------------------------------------------------------------------
# RAG Query Synthesis (generates answers from retrieved context)
# Task: Synthesize coherent answers from multiple document chunks
# Complexity: High | Speed: Less critical (user-facing, quality matters)
# Recommendation: Smarter model for better reasoning and coherence
# -----------------------------------------------------------------------------
RAG_LLM_MODEL=gpt-4o

# =============================================================================
# Chunking Configuration
# =============================================================================
CHUNK_SIZE_TOKENS=512
CHUNK_OVERLAP_TOKENS=50

# =============================================================================
# File Storage Paths
# =============================================================================
# Source data (user-provided, read-only) - supports subdirectories
DATA_SOURCE_DIR=./data/source

# Processed data (NCL-generated, temporary) - cleaned up after ingestion
DATA_PROCESSED_DIR=./data/processed

# Supabase Storage bucket for browsable archive - markdown previews and original files
# The bucket is auto-created on first use if it doesn't exist
ARCHIVE_BUCKET=archive

# Optional base URL for archive links (leave empty for relative /archive/ paths)
# Can be used for CDN URLs in production
ARCHIVE_BASE_URL=

# Folder structure:
# data/
# ├── source/           # Place your EML files here (can use subdirectories)
# │   ├── inbox/
# │   ├── emails/2024/
# │   └── ...
# └── processed/        # Temporary staging (auto-cleaned after ingestion)
#     └── extracted/    # Extracted ZIP contents (temporary)
#
# Supabase Storage bucket (archive):
# └── {doc_id}/         # Email markdown, original EML, and attachments
#     ├── email.eml.md      # Browsable email content
#     ├── email.eml         # Original for download
#     └── attachments/      # Attachment files + markdown previews

# =============================================================================
# Ingest Versioning
# =============================================================================
# Increment this when upgrading processing logic to trigger re-ingestion
# Use `ncl reprocess` to re-process documents with older versions
CURRENT_INGEST_VERSION=1

# =============================================================================
# Processing Options
# =============================================================================
BATCH_SIZE=10
MAX_CONCURRENT_FILES=5
MAX_CONCURRENT_EMBEDDINGS=5
EMBEDDING_BATCH_SIZE=100
ENABLE_OCR=true
ENABLE_PICTURE_DESCRIPTION=true

# =============================================================================
# ZIP Extraction Limits (DoS protection)
# =============================================================================
ZIP_MAX_FILES=100
ZIP_MAX_DEPTH=3
ZIP_MAX_TOTAL_SIZE_MB=500

# =============================================================================
# Reranker Configuration (improves search accuracy by 20-35%)
# =============================================================================
# Uses LiteLLM - supports multiple providers:
#   cohere/rerank-english-v3.0        - Cohere (default, requires COHERE_API_KEY)
#   cohere/rerank-multilingual-v3.0   - Cohere multilingual
#   together_ai/Salesforce/Llama-Rank-V1  - Together AI (requires TOGETHERAI_API_KEY)
#   deepinfra/Qwen/Qwen3-Reranker-0.6B    - DeepInfra (requires DEEPINFRA_API_KEY)
#   azure_ai/cohere-rerank-v3.5       - Azure AI (requires Azure credentials)
#   bedrock/amazon.rerank-v1:0        - AWS Bedrock (requires AWS credentials)
#   infinity/<model>                  - Self-hosted Infinity server
RERANK_ENABLED=true
RERANK_MODEL=cohere/rerank-english-v3.0
RERANK_TOP_N=5

# =============================================================================
# Display and Output
# =============================================================================
# Maximum characters of chunk content to display in source references
CHUNK_DISPLAY_MAX_CHARS=500

# Maximum error message length stored in database
ERROR_MESSAGE_MAX_LENGTH=1000

# =============================================================================
# API Configuration (for web interface)
# =============================================================================
# CORS origins (comma-separated for multiple origins)
CORS_ORIGINS=http://localhost:5173

# API server settings
API_HOST=0.0.0.0
API_PORT=8000

# =============================================================================
# Langfuse Observability (optional)
# =============================================================================
# LLM tracing and observability platform
# Get keys from: https://cloud.langfuse.com (EU) or https://us.cloud.langfuse.com (US)
LANGFUSE_ENABLED=false
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=
# EU: https://cloud.langfuse.com (default)
# US: https://us.cloud.langfuse.com
LANGFUSE_BASE_URL=https://cloud.langfuse.com
