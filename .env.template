# NCL Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# REQUIRED: User Input Needed (No Defaults)
# =============================================================================

# Supabase Configuration
# Get these from: https://supabase.com/dashboard → Project Settings
#
# SUPABASE_URL: Project URL (Settings → API → Project URL)
#   Example: https://abcdefghijkl.supabase.co
#
# SUPABASE_KEY: Service role key for backend operations (Settings → API → service_role secret)
#   WARNING: Keep this secret! Never expose in frontend code
#   Example: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
#
# SUPABASE_DB_URL: Direct PostgreSQL connection (Settings → Database → Connection string → URI)
#   Use the "Transaction pooler" connection string (port 5432) for best compatibility.
#   If you have IPv4 connectivity issues, use the "Session pooler" (port 5432) instead of direct connection.
#   Example: postgresql://postgres.[project-ref]:[password]@aws-0-[region].pooler.supabase.com:5432/postgres
#
SUPABASE_URL=
SUPABASE_KEY=
SUPABASE_DB_URL=

# OpenAI Configuration
# Get this from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# JWT Secret for API Authentication
# Get this from: Supabase Dashboard → Project Settings → API → JWT Secret
SUPABASE_JWT_SECRET=

# Cohere API Key (optional, but recommended for better search accuracy)
# Get this from: https://dashboard.cohere.com/api-keys
# Set RERANK_ENABLED=false below if you don't want to use reranking
COHERE_API_KEY=

# LlamaParse API Key (optional, for legacy document formats)
# Get this from: https://cloud.llamaindex.ai/api-key
# Enables support for: .doc, .xls, .ppt, .rtf, .csv, .odt, .ods, .odp
LLAMA_CLOUD_API_KEY=

# =============================================================================
# Model Configuration
# =============================================================================
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=1536
LLM_MODEL=gpt-5-mini

# =============================================================================
# Chunking Configuration
# =============================================================================
CHUNK_SIZE_TOKENS=512
CHUNK_OVERLAP_TOKENS=50

# Contextual chunking - LLM-generated summaries prepended to chunks
# Improves retrieval by 35-67% by adding document-level context
CONTEXT_LLM_MODEL=gpt-5-nano

# =============================================================================
# File Storage Paths
# =============================================================================
# Source data (user-provided, read-only) - supports subdirectories
DATA_SOURCE_DIR=./data/source

# Processed data (NCL-generated, temporary) - cleaned up after ingestion
DATA_PROCESSED_DIR=./data/processed

# Supabase Storage bucket for browsable archive - markdown previews and original files
# The bucket is auto-created on first use if it doesn't exist
ARCHIVE_BUCKET=archive

# Optional base URL for archive links (leave empty for relative /archive/ paths)
# Can be used for CDN URLs in production
ARCHIVE_BASE_URL=

# Folder structure:
# data/
# ├── source/           # Place your EML files here (can use subdirectories)
# │   ├── inbox/
# │   ├── emails/2024/
# │   └── ...
# └── processed/        # Temporary staging (auto-cleaned after ingestion)
#     └── extracted/    # Extracted ZIP contents (temporary)
#
# Supabase Storage bucket (archive):
# └── {doc_id}/         # Email markdown, original EML, and attachments
#     ├── email.eml.md      # Browsable email content
#     ├── email.eml         # Original for download
#     └── attachments/      # Attachment files + markdown previews

# =============================================================================
# Ingest Versioning
# =============================================================================
# Increment this when upgrading processing logic to trigger re-ingestion
# Use `ncl reprocess` to re-process documents with older versions
CURRENT_INGEST_VERSION=1

# =============================================================================
# Processing Options
# =============================================================================
BATCH_SIZE=10
MAX_CONCURRENT_FILES=5
MAX_CONCURRENT_EMBEDDINGS=5
EMBEDDING_BATCH_SIZE=100
ENABLE_OCR=true
ENABLE_PICTURE_DESCRIPTION=true

# =============================================================================
# ZIP Extraction Limits (DoS protection)
# =============================================================================
ZIP_MAX_FILES=100
ZIP_MAX_DEPTH=3
ZIP_MAX_TOTAL_SIZE_MB=500

# =============================================================================
# Reranker Configuration (improves search accuracy by 20-35%)
# =============================================================================
# Uses LiteLLM - supports multiple providers:
#   cohere/rerank-english-v3.0        - Cohere (default, requires COHERE_API_KEY)
#   cohere/rerank-multilingual-v3.0   - Cohere multilingual
#   together_ai/Salesforce/Llama-Rank-V1  - Together AI (requires TOGETHERAI_API_KEY)
#   deepinfra/Qwen/Qwen3-Reranker-0.6B    - DeepInfra (requires DEEPINFRA_API_KEY)
#   azure_ai/cohere-rerank-v3.5       - Azure AI (requires Azure credentials)
#   bedrock/amazon.rerank-v1:0        - AWS Bedrock (requires AWS credentials)
#   infinity/<model>                  - Self-hosted Infinity server
RERANK_ENABLED=true
RERANK_MODEL=cohere/rerank-english-v3.0
RERANK_TOP_N=5

# =============================================================================
# Display and Output
# =============================================================================
# Maximum characters of chunk content to display in source references
CHUNK_DISPLAY_MAX_CHARS=500

# Maximum error message length stored in database
ERROR_MESSAGE_MAX_LENGTH=1000

# =============================================================================
# API Configuration (for web interface)
# =============================================================================
# CORS origins (comma-separated for multiple origins)
CORS_ORIGINS=http://localhost:5173

# API server settings
API_HOST=0.0.0.0
API_PORT=8000

# =============================================================================
# Langfuse Observability (optional)
# =============================================================================
# LLM tracing and observability platform
# Get keys from: https://cloud.langfuse.com (EU) or https://us.cloud.langfuse.com (US)
LANGFUSE_ENABLED=false
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=
# EU: https://cloud.langfuse.com (default)
# US: https://us.cloud.langfuse.com
LANGFUSE_BASE_URL=https://cloud.langfuse.com
